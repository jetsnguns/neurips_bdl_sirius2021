{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"swag_pytorch.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.11"}},"cells":[{"cell_type":"markdown","metadata":{"id":"BYzpwF2FLeCH"},"source":["# Approximate Inference in Bayesian Deep Learning: Getting Started in Pytorch\n","\n","In this colab we will walk you through downloading the data, running your method and generating a submission for our NeurIPS 2021 competition. In this colab we use the Pytorch framework. For Jax see [this notebook](https://colab.research.google.com/drive/1SJ6waN8DOfby6qW9WWgJ0VLyaNzauhYD?usp=sharing).\n","\n","Useful references:\n","- [Competition website](https://izmailovpavel.github.io/neurips_bdl_competition/)\n","- [Efficient implementation of several baselines in JAX](https://github.com/google-research/google-research/tree/master/bnn_hmc)\n","- [Submission platform](https://competitions.codalab.org/competitions/33647)\n","\n","\n","## Setting up colab\n","\n","Colab provides an easy-to-use environment for working on the competition with access to free computational resources. However, you should also be able to run this notebook locally after installing the required dependencies. If you use colab, please select a `GPU` runtime type.\n","\n","## Preparing the data\n"]},{"cell_type":"code","metadata":{"id":"ii4uxN2hRv7c"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-07-30T18:12:45.075527Z","start_time":"2021-07-30T18:12:45.035591Z"},"id":"7idm1bY4SDUC"},"source":["import os\n","\n","print(os.getcwd())\n","os.chdir(\"/content/drive/MyDrive/neurips_bdl_sirius2021\")\n","print(os.getcwd())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-07-30T18:12:53.439468Z","start_time":"2021-07-30T18:12:46.783602Z"},"id":"MiSuC1jmhmDp","executionInfo":{"status":"ok","timestamp":1628148774084,"user_tz":-180,"elapsed":10829,"user":{"displayName":"qwerty","photoUrl":"","userId":"12432666219028155225"}}},"source":["import sys\n","import math\n","import matplotlib\n","import numpy as np\n","import copy\n","\n","import torch \n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, TensorDataset\n","import torch.optim as optim\n","from torch.optim.lr_scheduler import CyclicLR\n","from matplotlib import pyplot as plt\n","\n","from sklearn.model_selection import train_test_split\n","\n","sys.path.append(\"neurips_bdl_starter_kit\")\n","import pytorch_models as p_models\n","from loss_function import log_posterior_fn\n","from eval import get_accuracy_fn, evaluate_fn\n","from data_loading import safe_load_numpy, NormClfDataset\n","from vram_stats import get_vram_usage_str\n","import importlib\n","import swag\n","import averaging"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mrcvRuuJRGhZ"},"source":["We provide the datasets used in this competition in a public Google Cloud Storage bucket in the `.csv` format. Here we download the data:\n","\n","You can also download the data to your computer by clicking these links:\n","- [CIFAR-10 train features](https://storage.googleapis.com/neurips2021_bdl_competition/cifar10_train_x.csv)\n","- [CIFAR-10 train labels](https://storage.googleapis.com/neurips2021_bdl_competition/cifar10_train_y.csv)\n","- [CIFAR-10 test features](https://storage.googleapis.com/neurips2021_bdl_competition/cifar10_test_x.csv)\n","- [CIFAR-10 test labels](https://storage.googleapis.com/neurips2021_bdl_competition/cifar10_test_y.csv)"]},{"cell_type":"code","metadata":{"id":"da-9ecSDnKO9"},"source":["#!gsutil -m cp -r gs://neurips2021_bdl_competition/cifar10_*.csv ."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zDsZHMQzSGpZ"},"source":["We can now read the data and convert it into numpy arrays. This cell may take several minutes to run."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-07-30T18:12:59.274304Z","start_time":"2021-07-30T18:12:58.236243Z"},"id":"4o8NLOGgnVSV","executionInfo":{"status":"ok","timestamp":1628148789858,"user_tz":-180,"elapsed":15806,"user":{"displayName":"qwerty","photoUrl":"","userId":"12432666219028155225"}}},"source":["x_train = safe_load_numpy(\"cifar10_train_x\")\n","y_train = safe_load_numpy(\"cifar10_train_y\")\n","x_test = safe_load_numpy(\"cifar10_test_x\")\n","y_test = safe_load_numpy(\"cifar10_test_y\")"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"nmT8ymjz-NMr","executionInfo":{"status":"ok","timestamp":1628148791245,"user_tz":-180,"elapsed":1395,"user":{"displayName":"qwerty","photoUrl":"","userId":"12432666219028155225"}}},"source":["# Split for validation\n","train_size = 0.8\n","x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, train_size=train_size, stratify=y_train)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-07-28T07:37:08.016041Z","start_time":"2021-07-28T07:37:07.968187Z"},"id":"iQgkl3ZP1QzM","executionInfo":{"status":"ok","timestamp":1628148791253,"user_tz":-180,"elapsed":59,"user":{"displayName":"qwerty","photoUrl":"","userId":"12432666219028155225"}}},"source":["x_train = x_train.reshape((len(x_train), 32, 32, 3))\n","x_val = x_val.reshape((len(x_val), 32, 32, 3))\n","x_test = x_test.reshape((len(x_test), 32, 32, 3))"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-07-28T07:37:09.773650Z","start_time":"2021-07-28T07:37:08.032010Z"},"id":"i3nFA3RRyuwd"},"source":["plt.imshow(x_train[0].reshape(32, 32, 3))\n","plt.colorbar()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z8JIisIsSeOO"},"source":["Finally, we define a `torch.utils.data.TensorDataset` for the train and test datasets."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-07-28T07:37:15.704259Z","start_time":"2021-07-28T07:37:09.782652Z"},"id":"RZ3gwA1jmAJt","executionInfo":{"status":"ok","timestamp":1628148791263,"user_tz":-180,"elapsed":46,"user":{"displayName":"qwerty","photoUrl":"","userId":"12432666219028155225"}}},"source":["trainset = NormClfDataset(x_train, y_train)\n","valset = NormClfDataset(x_val, y_val)\n","testset = NormClfDataset(x_test, y_test)"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ui5dKavnSoQ9"},"source":["## Model and losses\n","\n","We provide the code for all the models used in the competition in the `neurips_bdl_starter_kit/torch_models.py` module. Here, we will load a ResNet-20 model with filter response normalization (FRN) and swish activations. The models are implemented in `pytorch`. \n","\n","We also define the cross-entropy likelihood (`log_likelihood_fn`) and Gaussian prior (`log_prior_fn`), and the corresponding posterior log-density (`log_posterior_fn`). The `log_posterior_wgrad_fn` computes the posterior log-density and its gradients with respect to the parameters of the model.\n","\n","The `evaluate_fn` function computes the accuracy and predictions of the model on a given dataset; we will use this function to generate the predictions for our submission."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-07-28T07:37:16.962404Z","start_time":"2021-07-28T07:37:15.947405Z"},"id":"gwMaZjf9yDTE"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-07-28T07:37:42.927747Z","start_time":"2021-07-28T07:37:24.824433Z"},"id":"1kbD51slyAU-"},"source":["net_fn =  p_models.get_model(\"resnet20_frn_swish\", data_info={\"num_classes\": 10})\n","if torch.cuda.is_available():\n","    print(\"GPU available!\")\n","    net_fn = net_fn.cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KEZnIWNmUEwE"},"source":["## Optimization and training \n","\n","In this colab we train an approximate maximum-a-posteriori (MAP) solution as our submission for simplicity. You can find efficient implementations of more advanced baselines in jax [here](https://github.com/google-research/google-research/tree/master/bnn_hmc).\n","\n","We use SGD with momentum. You can adjust the hyper-parameters or switch to a different optimizer by changing the code below.\n","\n","We run training for 5 epochs, which can take several minutes to complete. Note that in order to achieve good results you need to run the method substantially longer and tune the hyper-parameters."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-07-28T07:37:42.943359Z","start_time":"2021-07-28T07:37:31.167Z"},"id":"eKxlP6Iu1jmH","executionInfo":{"status":"ok","timestamp":1628148852167,"user_tz":-180,"elapsed":8,"user":{"displayName":"qwerty","photoUrl":"","userId":"12432666219028155225"}}},"source":["batch_size = 256\n","test_batch_size = 100\n","num_epochs = 1\n","momentum_decay = 0.8\n","lr = 0.001\n","\n","prior_variance = 5.\n"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"Eab0wCtlHPYZ","executionInfo":{"status":"ok","timestamp":1628148852843,"user_tz":-180,"elapsed":13,"user":{"displayName":"qwerty","photoUrl":"","userId":"12432666219028155225"}}},"source":["train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(valset, batch_size=test_batch_size, shuffle=False)\n","test_loader = DataLoader(testset, batch_size=test_batch_size, shuffle=False)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"CR0hb5y-HiIN","executionInfo":{"status":"ok","timestamp":1628148858524,"user_tz":-180,"elapsed":375,"user":{"displayName":"qwerty","photoUrl":"","userId":"12432666219028155225"}}},"source":["epoch_steps = len(train_loader)\n","optimizer = optim.Adam(net_fn.parameters(), lr=lr)\n","#optimizer = optim.SGD(net_fn.parameters(), lr=lr, momentum=momentum_decay)\n","#scheduler = CyclicLR(optimizer, base_lr=0.0001, max_lr=0.0005)"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-07-28T07:37:42.943359Z","start_time":"2021-07-28T07:37:33.480Z"},"id":"fLGw_AVJtjy3"},"source":["log_freq = 50\n","\n","for epoch in range(1, num_epochs + 1):\n","    running_loss = 0.0\n","    total_loss = 0.0\n","    for i, data in enumerate(train_loader):\n","        optimizer.zero_grad()\n","        loss = - log_posterior_fn(net_fn, data, prior_variance)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","        total_loss += loss.item()\n","\n","        train_accuracy_batch, _ = get_accuracy_fn(net_fn, data)\n","\n","        if i % log_freq == 0:    # print every 100 mini-batches\n","            print(f'[{epoch:3d}, {i + 1:5d}] loss: {running_loss/10:16.3f} acc: {train_accuracy_batch:.2f}')\n","            running_loss = 0.0\n","\n","    #scheduler.step()    \n","    print(\"Epoch {}\".format(epoch+1))\n","    print(\"\\tAverage loss: {}\".format(total_loss / epoch_steps))\n","    print(\"\\tTrain accuracy batch: {}\".format(train_accuracy_batch))\n","    if epoch % 4 == 0:\n","        val_acc, _ = evaluate_fn(net_fn, val_loader)\n","        print(\"\\tValidation accuracy: {}\".format(val_acc))\n","        print(get_vram_usage_str())\n","torch.save(net_fn.state_dict(), \"new.pth\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jXGQ9BtO5jZZ"},"source":["train_acc, all_train_probs = evaluate_fn(net_fn, train_loader, None)\n","print(\"\\tTrain accuracy: {}\".format(train_acc))\n","\n","test_acc, all_test_probs = evaluate_fn(net_fn, test_loader, None)\n","print(\"\\tTest accuracy: {}\".format(test_acc))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mgBrhmdwgYZh"},"source":["import swag\n","#importlib.reload(swag)\n","\n","T = 200\n","batch_size = 256\n","\n","teta, diag, D = swag.train_SWAG(net_fn, log_posterior_fn, trainset, T, \n","                                    batch_size, c=2, K=15)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZtceBLqeI-1m"},"source":["import averaging\n","#importlib.reload(averaging)\n","\n","S = 10\n","\n","all_test_probs = averaging.average_models(net_fn, test_loader, teta, diag, D, S)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L9KjyHdz7rSw"},"source":["all_test_probs_np = np.asarray(all_test_probs.cpu())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KGGLf-uMLwaL"},"source":["## Ensembles (Dropout version)"]},{"cell_type":"code","metadata":{"id":"yQkskhpuMQDU","executionInfo":{"status":"ok","timestamp":1628148821965,"user_tz":-180,"elapsed":836,"user":{"displayName":"qwerty","photoUrl":"","userId":"12432666219028155225"}}},"source":["import torch.nn.utils.convert_parameters as convert\n","\n","net_fn.load_state_dict(torch.load(\"adam_50epoch.pth\"))\n","net_fn.eval()\n","theta_init = convert.parameters_to_vector(net_fn.parameters())"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pn4P8wl-Obf9"},"source":["train_acc, all_train_probs = evaluate_fn(net_fn, train_loader, None)\n","print(\"\\tTrain accuracy: {}\".format(train_acc))\n","\n","test_acc, all_test_probs = evaluate_fn(net_fn, test_loader, None)\n","print(\"\\tTest accuracy: {}\".format(test_acc))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VC964WHdObf9"},"source":["import zeros\n","# importlib.reload(averaging)\n","\n","# def zeros(net_fn, log_posterior_fn, \n","#           trainset, valset, testset, \n","#           M, T=60, batch_size=128,\n","#           c=2, K=10, S=5,\n","#           lr=1e-4, prior_variance=1, momentum=0)\n","\n","M = [1000, 2000]\n","\n","all_test_probs_np = zeros.zeros(net_fn, log_posterior_fn, \n","                                trainset, valset, testset, \n","                                M, T=60, batch_size=128,\n","                                c=1, K=15, S=5,\n","                                lr=1e-4, prior_variance=1, momentum=0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QKDfvlHzxulX"},"source":["## Evaluating metrics"]},{"cell_type":"markdown","metadata":{"id":"EiQcDfma00Qn"},"source":["The starter kit comes with a script that can compute the agreement and total variation distance metrics used in the competition."]},{"cell_type":"code","metadata":{"id":"XNr_UlytxxB8","executionInfo":{"status":"ok","timestamp":1628149223760,"user_tz":-180,"elapsed":1023,"user":{"displayName":"qwerty","photoUrl":"","userId":"12432666219028155225"}}},"source":["import metrics"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zu-gQzoY05P2"},"source":["We can load the HMC reference predictions from the starter kit as well."]},{"cell_type":"code","metadata":{"id":"gdIagtdhxyRu","executionInfo":{"status":"ok","timestamp":1628149224266,"user_tz":-180,"elapsed":552,"user":{"displayName":"qwerty","photoUrl":"","userId":"12432666219028155225"}}},"source":["with open('data/cifar10/probs.csv', 'r') as fp:\n","  reference = np.loadtxt(fp)"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"80T57WFH09Eh"},"source":["Now we can compute the metrics!"]},{"cell_type":"code","metadata":{"id":"zw2PDdnzba0B"},"source":["from sklearn.metrics import accuracy_score\n","\n","swag_point = np.argmax(all_test_probs_np, axis=-1)\n","acc = accuracy_score(swag_point, y_test)\n","agree = metrics.agreement(all_test_probs_np, reference)\n","tvd = metrics.total_variation_distance(all_test_probs_np, reference)\n","\n","print(f\"SWAG \\t Acc: {acc:.2f} \\t Agreement: {agree:.2f} \\t TV distance: {tvd:.2f}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y7nYa4w3VmnO"},"source":["## Preparing the submission\n","\n","Once you run the code above, `all_test_probs` should contain an array of size `10000 x 10` where the rows correspond to test datapoints and columns correspond to classes."]},{"cell_type":"code","metadata":{"id":"atqsSbjBInpg"},"source":["all_test_probs.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FB2LhtfUW1ar"},"source":["Now, we need to save the array as `cifar10_probs.csv` and create a zip archive with this file."]},{"cell_type":"code","metadata":{"id":"h0CtqrJCI2-F"},"source":["np.savetxt(\"cifar10_probs.csv\", all_test_probs_np)\n","\n","!zip submission.zip cifar10_probs.csv"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f-3deTsUNcqI"},"source":["Finally, you can download the submission by running the code below. If the download doesn't start, check that your browser did not block it automatically."]},{"cell_type":"code","metadata":{"id":"ieCUqCPjJP0k"},"source":["from google.colab import files\n","files.download('submission.zip') "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"izLlUVM4XIWv"},"source":["Now you can head over to the [submission system](https://competitions.codalab.org/competitions/33512?secret_key=10f23c1f-9c86-4a7a-8406-d85b0a0713f2#participate) and upload your submission. Good luck :)"]}]}